{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3\n",
    "\n",
    "### UGBA 88: Data and Decisions\n",
    "\n",
    "<img src=\"what-ab-test.webp\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "This lab is designed to be completed in class. However, in case you need additional time, this assignment is due **Tuesday, October 22nd at 11:59pm**.\n",
    "\n",
    "The lab will be graded for **completion**. Lab office hours are held by Connector Assistants on Tuesdays after labs from 2-4pm in the DS Nexus in Moffitt.\n",
    "\n",
    "## Experiments and the Statistical Power of a Test\n",
    "\n",
    "The **statistical power** of an experiment reflects the likelihood of detecting a *statistically significant* treatment effect for the outcome of interest if a treatment effect is present. The purpose of this lab to explore how the statistical power of an experiment depends on sample size and the true effect size. Statistical power is an essential consideration when designing an experiment because experiments without adequate power may not teach us much about the causal effect of the treatment.\n",
    "\n",
    "### Outline\n",
    "\n",
    "*Dependencies:* \n",
    "- datascience\n",
    "- Numpy\n",
    "- Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import numpy as np\n",
    "\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab3.ok')\n",
    "_ = ok.auth(inline=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a New Landing Page\n",
    "\n",
    "Our motivating application in this lab is a common one for online companies: testing a new design for a webpage. \n",
    "\n",
    "Consider a company that provides some service to customers. Potential customers that click on an advertisement or referral link are redirected to a landing page that describes the service in more detail. Most importantly, potential customers can *sign up* for the service via the landing page.\n",
    "\n",
    "<img src=\"slack_landing.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<center>a Slack landing page</center>\n",
    "\n",
    "The landing page is an important path for generating new business, but some in the company believe the page is too confusing and may be an impediment to customer growth. The company's designers have redesigned the landing page in an attempt to increase user engagement. The outcome of interest is whether a user that arrives at the landing page actually signs up for the service. We'll call this outcome the **clickthrough rate**.\n",
    "\n",
    "In practice, most companies will test whether a new design is better than the old one by running an A/B test. That is, they would run an experiment that randomly assigned some users to the old landing page and others to the new landing page, and then compare outcomes. In this lab, for the sake of simplicity, we'll compare the new design against a *fixed* benchmark. The benchmark we'll use is **20%**. You can think of this as the *historic* clickthrough rate for the old design. In the experiment we will randomly assign a set of users to see the new design. If we have sufficient evidence that the clickthrough rate for the new design would be higher than 20%, we'll call the new design a success and launch it to all users.\n",
    "\n",
    "## Section 1: Review of Hypothesis Testing\n",
    "\n",
    "As you may recall from Data 8, we use **hypothesis tests** to choose between two views about how data are generated. These two views are called the **null** and **alternative** hypotheses.\n",
    "\n",
    "For reference, I've reprinted the Data 8 definitions of the null and alternative hypotheses below.\n",
    "\n",
    "**The null hypothesis** is a clearly defined model about chances. It says that the data were generated at random under clearly specified assumptions about the randomness. The word \"null\" reinforces the idea that if the data look different from what the null hypothesis predicts, the difference is due to nothing but chance.\n",
    "\n",
    "**The alternative hypothesis** says that some reason other than chance made the data differ from the predictions of the model in the null hypothesis.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Q1.1** State the null and alternative hypotheses for our landing page experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*write answer here*\n",
    "\n",
    "Null hypothesis: ...\n",
    "\n",
    "Alternative hypothesis: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Hypothesis Test\n",
    "\n",
    "Remember the general flavor of a hypothesis test from Data 8:\n",
    "- Choose a test statistic\n",
    "- Simulate the empirical distribution of the test statistic under the null hypothesis\n",
    "- Compare the observed test statistic to this empirical distribution\n",
    "- From this comparison, determine whether the null hypothesis is supported or not\n",
    "\n",
    "A natural test statistic to use here is the **clickthrough rate**. We will now build towards simulating the empirical distribution of the clickthrough rate under the null hypothesis.\n",
    "\n",
    "**Q1.2** Run the cell below, which initializes the `sample_proportion` function.\n",
    "\n",
    "This function takes as input two values, the sample size and the population probability of success, and returns the success rate for a random sample drawn from the pouplation. (Note: this is a slight simplification of the function `sample_proportions` from Data 8.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_proportion(n, prob):\n",
    "    \"\"\"\n",
    "    n (int): the sample size to be taken from\n",
    "    prob (float):  probability of success\n",
    "    \"\"\"\n",
    "    if (prob > 1):\n",
    "        return 'probability of success must not exceed 1'\n",
    "    if (prob < 0):\n",
    "        return 'probability of success must be positive'\n",
    "    return np.random.binomial(n, prob) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3** Let's draw one sample of the data under the null hypothesis using the `sample_proportion` function.\n",
    "\n",
    "Suppose our sample size is 100 customers. Compare the clickthrough rate of the data simulated under the null with the expected clickthrough rate under the null. Notice a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample size\n",
    "n = 100\n",
    "\n",
    "#success rate under null hypothesis (a value of 0.2 corresponds to 20% clickthrough rate)\n",
    "null_clickthrough_rate = 0.2\n",
    "\n",
    "#simulate values for clickthrough rate under the null\n",
    "#hint: use sample_proportion function defined above\n",
    "observed_clickthrough_rate = ...\n",
    "\n",
    "print(\"Expected Clickthrough Rate:\", null_clickthrough_rate)\n",
    "print(\"Observed Clickthrough Rate:\", observed_clickthrough_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try re-running the cell a few times. Notice that the simulated clickthrough rate generally does not match the expected clickthrough rate exactly, but is usually close.\n",
    "\n",
    "We'll call the latest clickthrough rate you simulate the **observed clickthrough rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating an Empirical Distribution for the Null\n",
    "\n",
    "Next, we will simulate an empirical distribution of the clickthrough rate for samples of size 100 under the null hypothesis. This will give us a sense of how variable the clickthrough rate could be. We will generate 10,000 samples and calculate the clickthrough rate for each sample.\n",
    "\n",
    "**Q1.4** Fill out the function below that returns an array of the clickthrough rates of simulated trials. Then run the cell below to ensure that you get reasonable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of simulated draws for empirical distribution\n",
    "model_draws = 10000\n",
    "\n",
    "def simulate_n_trials(num_trials, prob, sample_size):\n",
    "    \"\"\"\n",
    "    num_trials: the number of trials to record empirical success rates from\n",
    "    prob: the probability of success under the model\n",
    "    sample_size: the size of the population that you use to simulate success rates\n",
    "    \"\"\"\n",
    "    model_clickthrough_rates = make_array()\n",
    "    for i in np.arange(num_trials):\n",
    "        sim_model_clickthrough_rate = ...\n",
    "        model_clickthrough_rates = np.append(model_clickthrough_rates, sim_model_clickthrough_rate)\n",
    "    return model_clickthrough_rates\n",
    "\n",
    "model_clickthrough_rates = simulate_n_trials(model_draws, null_clickthrough_rate, n)\n",
    "model_clickthrough_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Data under the Null\n",
    "\n",
    "To evaluate data under the null, we will start by plotting a histogram of the empirical distribution. Then we will check how your observed clickthrough rate compares.\n",
    "\n",
    "**Q1.5** Make a histogram of the simulated clickthrough rates.\n",
    "\n",
    "For comparison, the code below will add a red dot for your `observed_clickthrough_rate` value from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a histogram of model_clickthrough_rates values\n",
    "...\n",
    "\n",
    "#this line just draws a red dot corresponding to the observed clickthrough rate\n",
    "plt.scatter(observed_clickthrough_rate, 0, color='red', s=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was the clickthrough rate you simulated significantly different from what we would predict under the null? \n",
    "\n",
    "### The P-Value\n",
    "\n",
    "To be more precise about what we mean by \"significance\", we define the *p-value* in Data 8. I've reprinted the Data 8 definition below:\n",
    "\n",
    "\"**The P-value** is the chance, based on the model in the null hypothesis, that the test statistic is equal to the value that was observed in the data or is even further in the direction of the alternative.\n",
    "\n",
    "If a P-value is small, that means the tail beyond the observed statistic is small and so the observed statistic is far away from what the null predicts. This implies that the data support the alternative hypothesis better than they support the null.\"\n",
    "\n",
    "In other words, when the p-value is small, we **reject** the null hypothesis.\n",
    "\n",
    "Let's compute the p-value corresponding to your observed clickthrough rate. This is the share of values in the array `clickthrough_rates` that are greater than or equal to `observed_clickthrough_rate`.\n",
    "\n",
    "[Link to relevant data 8 material](https://www.inferentialthinking.com/chapters/11/3/decisions-and-uncertainty.html#Conventional-Cut-offs-and-the-P-value)\n",
    "\n",
    "**Q1.6** Calculate the p-value for your observed clickthrough rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the p-value\n",
    "p_value = ...\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine whether an observed statistic is 'significant', a common cutoff to use is a p-value of 0.05 or below. More specifically, we would say an observed statistic with a p-value of 0.05 or below under the null is 'statistically significant at the 5% significance level'.\n",
    "\n",
    "How large a clickthrough rate would we have needed to observe to declare it statistically significant and reject the null hypothesis? We can compute this by taking the 95th percentile of our empirical distribution of the clickthrough rate under the null. By definition, only 5% of values in the empirical distribution are larger than this cutoff.\n",
    "\n",
    "*Run the cell below to calculate the cutoff for a significance level of 5%.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = percentile(95, model_clickthrough_rates)\n",
    "cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your cutoff should be around 0.27.\n",
    "\n",
    "### Simulating Experiment Results\n",
    "\n",
    "The cutoff above defines a decision rule. If our experiment produces a result that's above the cutoff, we launch the new design.\n",
    "\n",
    "In this section we will ask the following question: if the null hypothesis is true and the clickthrough rate for the new design is indeed 20%, how often will we reject the null hypothesis and launch the new design? \n",
    "\n",
    "We will assume the clickthrough rate for the new design is also 20%, and simulate experimental results under that distribution. A simulated experiment here is a random draw of 100 observations. For each draw, we will apply our decision rule and decide whether to reject the null hypothesis. Then, across all our simulations, we will measure how often we reject the null hypothesis.\n",
    "\n",
    "We'll simulate 1,000 experiment results given this clickthrough rate. For each draw, let's also store the *p-value* and whether or not the null hypothesis is rejected.\n",
    "\n",
    "**Q1.7** Simulate 1,000 experiments, store the p-values of each experiment and whether or not we would reject the null. Store the results in `experiments_table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set clickthrough rate for the new design\n",
    "#at first we'll set this to the null clickthrough rate\n",
    "true_clickthrough_rate = null_clickthrough_rate\n",
    "\n",
    "#number of experiments we'll simulate\n",
    "exp_draws = 1000\n",
    "\n",
    "#simulate experiments\n",
    "simulated_data = simulate_n_trials(..., true_clickthrough_rate, n)\n",
    "\n",
    "#initiate arrays to store p-value and whether null is rejected for each simulated experiment\n",
    "p_values = make_array()\n",
    "rejections = make_array()\n",
    "\n",
    "#loop through elements of simulated_data\n",
    "for simulated_val in simulated_data:\n",
    "    #calculate p-value\n",
    "    p_value = ...\n",
    "    #add p_value to array\n",
    "    p_values = np.append(p_values, p_value)\n",
    "    #calculate indicator for whether we reject the null hypothesis\n",
    "    rejection = ...\n",
    "    #add rejection to array\n",
    "    rejections = np.append(rejections, rejection)\n",
    "\n",
    "#store results in table, one row per experiment\n",
    "experiments_table = Table().with_columns(\"simulated_values\", simulated_data, \"p-values\", p_values, \"rejections\", rejections) \n",
    "experiments_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade(\"q1_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at the distribution of experiment outcomes. First we'll look at the distribution of p-values across all of our simulated experiments.\n",
    "\n",
    "**Q1.8** Produce a histogram of the p-values for the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the null hypothesis is true, we get a variety of p-values. In fact, we may actually **reject** the null hypothesis some of the time.\n",
    "\n",
    "**Q1.9** Across simulated experiments, how often do we reject the null hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the rate that null hypothesis is rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when the null hypothesis is true, we will reject the null about **5%** of the time (your number may differ slightly).\n",
    "\n",
    "### False-Positives\n",
    "\n",
    "That value should sound familiar. We applied a 5% significance level, or set our cutoff for 'significance' at $p \\le 0.05$. \n",
    "\n",
    "Recall that, in our example, the null hypothesis is in fact true. So if we decide to reject the null hypothesis and launch the design change, we are in fact making a *mistake*. We call this type of mistake a **false-positive** or a **Type I** error. Our experiment gave us a significant result, but it occurred by chance, and not because the new design actually pushed the clickthrough rate above 20%.\n",
    "\n",
    "When we set our cutoff at $p = 0.05$ ('5% significance level'), we are setting a decision rule where, if the null hypothesis is true, we'll hope to make this type of mistake at most 5% of the time.\n",
    "\n",
    "Depending on how costly a false-positive is, we may decide to set the cutoff at a higher or lower value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: False-Negatives\n",
    "\n",
    "Suppose the true clickthrough rate is instead **25%**. In this case, the clickthrough rate is large enough that the *correct* decision is to launch the new design. If we fail to reject the null hypothesis and do not launch the new design, we are making a different kind of mistake. In this case, we call the mistake a **false-negative** or a **Type II** error. \n",
    "\n",
    "Even if the null hypothesis is false, the evidence may not be strong enough for us to (correctly) reject the null hypothesis. As you calculated above, we will reject the null hypothesis if the clickthrough rate we calculate is above **27%**. So if the true clickthrough rate is 25%, it's quite possible we will draw a sample that does not provide strong enough evidence to reject the null hypothesis.\n",
    "\n",
    "The **power** of a statistical test is 1 minus the probability of a **false-negative**. In other words, it is the probability of correctly rejecting the null hypothesis when it is false. Note that the power depends on the specific value for the true clickthrough rate that we're considering. In this case, the value we're considering is 25%. A experiment with low power is one that's unlikely to produce enough evidence to reject the null hypothesis even if it's in fact false. In the context of our landing page, with a low power experiment we will have trouble identifying new designs that are in fact effective, missing opportunities for improving clickthrough rates.\n",
    "\n",
    "**Q2.1** Let's calculate the power of our test. First, simulate the distribution of clickthrough rates you will observe if the true clickthrough rate is 25%. As above, simulate 1,000 experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set success rate for the new design\n",
    "true_clickthrough_rate = 0.25\n",
    "\n",
    "#simulate values for observed clickthrough rates using true_clickthrough_rate value\n",
    "#remember: simulate for 1,000 experiments (use exp_draws value)\n",
    "true_clickthrough_rates = ...\n",
    "true_clickthrough_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2** Plot histogram of simulated clickthrough rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of simulated clickthrough rates\n",
    "...\n",
    "\n",
    "#code below draws a vertical line at the cutoff for rejecting the null hypothesis\n",
    "#note: `cutoff` is already defined above\n",
    "plt.axvline(x= cutoff, color='red', lw = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for many simulated experiments, the clickthrough rate we will observe is below the cutoff for rejecting the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3** Below, calculate the rate at which you would reject the null hypothesis if the true clickthrough rate is in fact 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate rejection rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find a value of about 30%. That means that, if our new landing page improves the clickthrough rate from 20% to 25%, there is about **70%** chance of us committing a Type II error, meaning we fail to the reject the null hypothesis and adopt the new and improved design!\n",
    "\n",
    "How can we reduce the Type II error rate? There are two approaches: (1) we can increase the p-value cutoff we use in rejecting the null hypothesis or (2) we can increase the sample size. Increasing the p-value cutoff increases the power of our test by reducing our threshold for rejecting the null hypothesis. Increasing the sample size helps both by reducing our threshold for rejecting the null hypothesis (for a fixed p-value cutoff) and by making it more likely that the clickthrough rate we observe in the experiment is close to the true clickthrough rate.\n",
    "\n",
    "\n",
    "Both involve tradeoffs. Increasing the significance level of our test will increase our chances of getting a false positive. Increasing our sample size is usually costly--in this case, it will take longer to accumulate enough users for inclusion in the experiment.\n",
    "\n",
    "Let's investigate how the statistical power of our test changes with the sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.4** Calculate the rate at which you would reject the null hypothesis for a range of sample size. We suggest looking at sample sizes ranging from 20 to 1000 and have provided code for generating a list with this range, but you are welcome to change those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, we create an array of sample size values that you will loop through\n",
    "#you can change these values if you would like\n",
    "\n",
    "#set number of sample sizes to try\n",
    "num_sizes = 20\n",
    "\n",
    "#set minimum and maximum sample size\n",
    "n_min = 20\n",
    "n_max = 1000\n",
    "\n",
    "#generate sequence of sample sizes ranging from n_min to n_max\n",
    "row_num = np.arange(0, num_sizes)\n",
    "\n",
    "#note: np.floor ensures that we get an integer value for the sample size\n",
    "sample_sizes = np.floor(n_min + ((n_max - n_min)/(num_sizes - 1))*row_num)\n",
    "sample_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate null rejection rates for various sample sizes\n",
    "\n",
    "#create array of rejection rates for storage\n",
    "reject_rates = make_array()\n",
    "\n",
    "#create array of cutoff values for storage\n",
    "cutoff_values = make_array()\n",
    "\n",
    "#loop through sample sizes\n",
    "for n in sample_sizes:\n",
    "\n",
    "    #simulate empirical distribution for observed clickthrough rates *under the null hypothesis*\n",
    "    model_clickthrough_rates = ...\n",
    "    \n",
    "    #calculate cutoff for this sample size\n",
    "    cutoff_n = ...\n",
    "    \n",
    "    #store cutoff values for rejecting null hypothesis\n",
    "    cutoff_values = np.append(cutoff_values, cutoff_n)\n",
    "\n",
    "    #simulate empirical distribution for observed clickthrough rates using true_clickthrough_rate value\n",
    "    #note: as above, simulate 1,000 experiments (use exp_draws value)\n",
    "    true_clickthrough_rates = ...\n",
    "    \n",
    "    #calculate the rate at which the null hypothesis is rejected\n",
    "    reject_rate = ...\n",
    "\n",
    "    #store reject rate for given sample size\n",
    "    reject_rates = np.append(reject_rates, reject_rate)\n",
    "\n",
    "reject_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cutoff values are *decreasing* in sample size. With a bigger sample, we don't need to observe as large a clickthrough rate to conclude that the null hypothesis (that the true clickthrough rate is 20%) is unlikely to be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this see cutoff values\n",
    "cutoff_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.5** Run the cell below to plot statistical **power** as a function of sample size (when the true clickthrough rate is 25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot rejection rates as function of sample size\n",
    "\n",
    "plt.scatter(sample_sizes, reject_rates)\n",
    "#plt.ylim(ymin= 0.0, ymax = 1.0)\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Test Power (Null Rejection Rate)')\n",
    "plt.title('Sample Size and Rejection Rates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.6** Describe the pattern you see. How does the power of the test change with the sample size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.7** Next, let's see how statistical power varies with the true clickthrough rate. As before, fix the sample size at 100. In the code provided below, we allow the true clickthrough rate to range from 21% to 35%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, we create an array of true clickthrough rate values that you will loop through\n",
    "#you can change these values if you would like\n",
    "\n",
    "#revert to original sample size\n",
    "n = 100\n",
    "\n",
    "#set number of true clickthrough rate values to try\n",
    "num_true_rates = 15\n",
    "\n",
    "#set minimum and maxium sample size\n",
    "r_min = 0.21\n",
    "r_max = 0.35\n",
    "\n",
    "#generate sequence of sample sizes ranging from r_min to r_max\n",
    "row_num = np.arange(0, num_true_rates)\n",
    "true_rates = r_min + ((r_max - r_min)/(num_true_rates - 1))*row_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create array of rejection rates for storage\n",
    "reject_rates = make_array()\n",
    "\n",
    "#loop through values of true_clickthrough_rate\n",
    "for r in true_rates:\n",
    "    #simulate empirical distribution for observed clickthrough rate using given value from true_rates array\n",
    "    #note: as above, simulate 1,000 experiments (use exp_draws value)\n",
    "    true_clickthrough_rates = ...\n",
    "    \n",
    "    #calculate rejection rate for given value from true_rates array\n",
    "    #note: can use previously defined `cutoff`, which was measured for n = 100\n",
    "    reject_rate = ...\n",
    "\n",
    "    #store reject rate for given sample size\n",
    "    reject_rates = np.append(reject_rates, reject_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.8** Run the cell below to plot statistical power as a function of the true clickthrough rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot rejection rates as function of true clickthrough rate\n",
    "plt.scatter(true_rates, reject_rates)\n",
    "plt.xlabel('True Clickthrough Rate')\n",
    "plt.ylabel('Test Power (Null Rejection Rate)')\n",
    "plt.title('True Clickthrough Rate and Rejection Rates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.9** Describe the pattern you see. How does the power of the test change with the true clickthrough rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion: when designing an experiment, it is important to consider the magnitude of the treatment effect you would like to be able to detect, and to choose a sample size accordingly. It is standard to aim for a sample size that provides 80% power for a target effect size. In this case, if that target effect size is in fact the size of the true treatment effect, you will (correctly) reject the null hypothesis of no treatment effect *80% of the time.* From **Q2.4**, you should see that we need a sample size of about 400 to have 80% power if the true clickthrough rate is 0.25. You can measure this for other values of the true clickthrough rate by changing the value for `true_clickthrough_rate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Congratulations, you've finished Lab 3! To submit the lab, run the two cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For your convenience, you can run this cell to run all the tests at once\n",
    "import os\n",
    "_ = [ok.grade(q[:-3]) for q in os.listdir(\"tests\") if q.startswith('q')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
